{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images=[]\n",
    "emo_label={0:'sad', 1:'happy', 2:'angry', 3:'disgust', 4:'surprise', 5:'fear', 6:'neutral'}\n",
    "gender_label={0:'man',2:'woman'}\n",
    "test_model=MultiTaskModel(phase='test')\n",
    "test_model.load_state_dict(best_model_wts)\n",
    "test_model.eval()\n",
    "a=['ss.png','smile_man.png','angry_woman.png']\n",
    "for image in a:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        img=cv2.imread(image)\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img=cv2.resize(img,(128,128))\n",
    "        img=np.transpose(img,(2,0,1))\n",
    "        img=torch.FloatTensor(img).to(device)\n",
    "        img=torch.unsqueeze(img,0)/255.0\n",
    "        \n",
    "        start=time.time()\n",
    "        gender_output,emotion_output=model(img)\n",
    "        infer_time=time.time()-start\n",
    "        \n",
    "        emo_pred=emotion_output.argmax(1,keepdim=True)\n",
    "        gender_pred=gender_output.argmax(1,keepdim=True)\n",
    "        emotion=emo_label[emo_pred.item()]\n",
    "        gender=gender_label[gender_pred.item()]\n",
    "    example_images.append(wandb.Image(\n",
    "                    img, caption=f'Pred:{gender},{emotion}, inference_time:{infer_time:.4f}'))\n",
    "wandb.log({\"Image\": example_images})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
