{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import wandb\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from MTL.model import resnet,MTL_model\n",
    "import glob\n",
    "from torchvision.models import  resnet18\n",
    "import torch.nn as nn\n",
    "from data.datasets import EmotionDataset,GenderDataset,AgeDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_label={0:'sad', 1:'happy', 2:'angry', 3:'disgust', 4:'surprise', 5:'fear', 6:'neutral'}\n",
    "gender_label={0:'man',1:'woman'}\n",
    "age_label={0:'youth', 1: 'student', 2: 'adult', 3:'elder'}\n",
    "\n",
    "test_gender_dataset=GenderDataset(phase='test')\n",
    "test_emo_dataset=EmotionDataset(phase='test')\n",
    "test_age_dataset=AgeDataset(phase='test')\n",
    "\n",
    "test_gender_loader=DataLoader(test_gender_dataset,batch_size=16,shuffle=True,num_workers=1)\n",
    "test_emo_loader=DataLoader(test_emo_dataset,batch_size=16,shuffle=True,num_workers=1)\n",
    "test_age_loader=DataLoader(test_age_dataset,batch_size=16,shuffle=True,num_workers=1)\n",
    "device='cuda:3' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EmotionModel:\n\tMissing key(s) in state_dict: \"feature_extractor.layer1.0.conv3.weight\", \"feature_extractor.layer1.0.bn3.weight\", \"feature_extractor.layer1.0.bn3.bias\", \"feature_extractor.layer1.0.bn3.running_mean\", \"feature_extractor.layer1.0.bn3.running_var\", \"feature_extractor.layer1.0.downsample.0.weight\", \"feature_extractor.layer1.0.downsample.1.weight\", \"feature_extractor.layer1.0.downsample.1.bias\", \"feature_extractor.layer1.0.downsample.1.running_mean\", \"feature_extractor.layer1.0.downsample.1.running_var\", \"feature_extractor.layer1.1.conv3.weight\", \"feature_extractor.layer1.1.bn3.weight\", \"feature_extractor.layer1.1.bn3.bias\", \"feature_extractor.layer1.1.bn3.running_mean\", \"feature_extractor.layer1.1.bn3.running_var\", \"feature_extractor.layer1.2.conv1.weight\", \"feature_extractor.layer1.2.bn1.weight\", \"feature_extractor.layer1.2.bn1.bias\", \"feature_extractor.layer1.2.bn1.running_mean\", \"feature_extractor.layer1.2.bn1.running_var\", \"feature_extractor.layer1.2.conv2.weight\", \"feature_extractor.layer1.2.bn2.weight\", \"feature_extractor.layer1.2.bn2.bias\", \"feature_extractor.layer1.2.bn2.running_mean\", \"feature_extractor.layer1.2.bn2.running_var\", \"feature_extractor.layer1.2.conv3.weight\", \"feature_extractor.layer1.2.bn3.weight\", \"feature_extractor.layer1.2.bn3.bias\", \"feature_extractor.layer1.2.bn3.running_mean\", \"feature_extractor.layer1.2.bn3.running_var\", \"feature_extractor.layer2.0.conv3.weight\", \"feature_extractor.layer2.0.bn3.weight\", \"feature_extractor.layer2.0.bn3.bias\", \"feature_extractor.layer2.0.bn3.running_mean\", \"feature_extractor.layer2.0.bn3.running_var\", \"feature_extractor.layer2.1.conv3.weight\", \"feature_extractor.layer2.1.bn3.weight\", \"feature_extractor.layer2.1.bn3.bias\", \"feature_extractor.layer2.1.bn3.running_mean\", \"feature_extractor.layer2.1.bn3.running_var\", \"feature_extractor.layer2.2.conv1.weight\", \"feature_extractor.layer2.2.bn1.weight\", \"feature_extractor.layer2.2.bn1.bias\", \"feature_extractor.layer2.2.bn1.running_mean\", \"feature_extractor.layer2.2.bn1.running_var\", \"feature_extractor.layer2.2.conv2.weight\", \"feature_extractor.layer2.2.bn2.weight\", \"feature_extractor.layer2.2.bn2.bias\", \"feature_extractor.layer2.2.bn2.running_mean\", \"feature_extractor.layer2.2.bn2.running_var\", \"feature_extractor.layer2.2.conv3.weight\", \"feature_extractor.layer2.2.bn3.weight\", \"feature_extractor.layer2.2.bn3.bias\", \"feature_extractor.layer2.2.bn3.running_mean\", \"feature_extractor.layer2.2.bn3.running_var\", \"feature_extractor.layer2.3.conv1.weight\", \"feature_extractor.layer2.3.bn1.weight\", \"feature_extractor.layer2.3.bn1.bias\", \"feature_extractor.layer2.3.bn1.running_mean\", \"feature_extractor.layer2.3.bn1.running_var\", \"feature_extractor.layer2.3.conv2.weight\", \"feature_extractor.layer2.3.bn2.weight\", \"feature_extractor.layer2.3.bn2.bias\", \"feature_extractor.layer2.3.bn2.running_mean\", \"feature_extractor.layer2.3.bn2.running_var\", \"feature_extractor.layer2.3.conv3.weight\", \"feature_extractor.layer2.3.bn3.weight\", \"feature_extractor.layer2.3.bn3.bias\", \"feature_extractor.layer2.3.bn3.running_mean\", \"feature_extractor.layer2.3.bn3.running_var\", \"feature_extractor.layer2.4.conv1.weight\", \"feature_extractor.layer2.4.bn1.weight\", \"feature_extractor.layer2.4.bn1.bias\", \"feature_extractor.layer2.4.bn1.running_mean\", \"feature_extractor.layer2.4.bn1.running_var\", \"feature_extractor.layer2.4.conv2.weight\", \"feature_extractor.layer2.4.bn2.weight\", \"feature_extractor.layer2.4.bn2.bias\", \"feature_extractor.layer2.4.bn2.running_mean\", \"feature_extractor.layer2.4.bn2.running_var\", \"feature_extractor.layer2.4.conv3.weight\", \"feature_extractor.layer2.4.bn3.weight\", \"feature_extractor.layer2.4.bn3.bias\", \"feature_extractor.layer2.4.bn3.running_mean\", \"feature_extractor.layer2.4.bn3.running_var\", \"feature_extractor.layer2.5.conv1.weight\", \"feature_extractor.layer2.5.bn1.weight\", \"feature_extractor.layer2.5.bn1.bias\", \"feature_extractor.layer2.5.bn1.running_mean\", \"feature_extractor.layer2.5.bn1.running_var\", \"feature_extractor.layer2.5.conv2.weight\", \"feature_extractor.layer2.5.bn2.weight\", \"feature_extractor.layer2.5.bn2.bias\", \"feature_extractor.layer2.5.bn2.running_mean\", \"feature_extractor.layer2.5.bn2.running_var\", \"feature_extractor.layer2.5.conv3.weight\", \"feature_extractor.layer2.5.bn3.weight\", \"feature_extractor.layer2.5.bn3.bias\", \"feature_extractor.layer2.5.bn3.running_mean\", \"feature_extractor.layer2.5.bn3.running_var\", \"feature_extractor.layer2.6.conv1.weight\", \"feature_extractor.layer2.6.bn1.weight\", \"feature_extractor.layer2.6.bn1.bias\", \"feature_extractor.layer2.6.bn1.running_mean\", \"feature_extractor.layer2.6.bn1.running_var\", \"feature_extractor.layer2.6.conv2.weight\", \"feature_extractor.layer2.6.bn2.weight\", \"feature_extractor.layer2.6.bn2.bias\", \"feature_extractor.layer2.6.bn2.running_mean\", \"feature_extractor.layer2.6.bn2.running_var\", \"feature_extractor.layer2.6.conv3.weight\", \"feature_extractor.layer2.6.bn3.weight\", \"feature_extractor.layer2.6.bn3.bias\", \"feature_extractor.layer2.6.bn3.running_mean\", \"feature_extractor.layer2.6.bn3.running_var\", \"feature_extractor.layer2.7.conv1.weight\", \"feature_extractor.layer2.7.bn1.weight\", \"feature_extractor.layer2.7.bn1.bias\", \"feature_extractor.layer2.7.bn1.running_mean\", \"feature_extractor.layer2.7.bn1.running_var\", \"feature_extractor.layer2.7.conv2.weight\", \"feature_extractor.layer2.7.bn2.weight\", \"feature_extractor.layer2.7.bn2.bias\", \"feature_extractor.layer2.7.bn2.running_mean\", \"feature_extractor.layer2.7.bn2.running_var\", \"feature_extractor.layer2.7.conv3.weight\", \"feature_extractor.layer2.7.bn3.weight\", \"feature_extractor.layer2.7.bn3.bias\", \"feature_extractor.layer2.7.bn3.running_mean\", \"feature_extractor.layer2.7.bn3.running_var\", \"feature_extractor.layer3.0.conv3.weight\", \"feature_extractor.layer3.0.bn3.weight\", \"feature_extractor.layer3.0.bn3.bias\", \"feature_extractor.layer3.0.bn3.running_mean\", \"feature_extractor.layer3.0.bn3.running_var\", \"feature_extractor.layer3.1.conv3.weight\", \"feature_extractor.layer3.1.bn3.weight\", \"feature_extractor.layer3.1.bn3.bias\", \"feature_extractor.layer3.1.bn3.running_mean\", \"feature_extractor.layer3.1.bn3.running_var\", \"feature_extractor.layer3.2.conv1.weight\", \"feature_extractor.layer3.2.bn1.weight\", \"feature_extractor.layer3.2.bn1.bias\", \"feature_extractor.layer3.2.bn1.running_mean\", \"feature_extractor.layer3.2.bn1.running_var\", \"feature_extractor.layer3.2.conv2.weight\", \"feature_extractor.layer3.2.bn2.weight\", \"feature_extractor.layer3.2.bn2.bias\", \"feature_extractor.layer3.2.bn2.running_mean\", \"feature_extractor.layer3.2.bn2.running_var\", \"feature_extractor.layer3.2.conv3.weight\", \"feature_extractor.layer3.2.bn3.weight\", \"feature_extractor.layer3.2.bn3.bias\", \"feature_extractor.layer3.2.bn3.running_mean\", \"feature_extractor.layer3.2.bn3.running_var\", \"feature_extractor.layer3.3.conv1.weight\", \"feature_extractor.layer3.3.bn1.weight\", \"feature_extractor.layer3.3.bn1.bias\", \"feature_extractor.layer3.3.bn1.running_mean\", \"feature_extractor.layer3.3.bn1.running_var\", \"feature_extractor.layer3.3.conv2.weight\", \"feature_extractor.layer3.3.bn2.weight\", \"feature_extractor.layer3.3.bn2.bias\", \"feature_extractor.layer3.3.bn2.running_mean\", \"feature_extractor.layer3.3.bn2.running_var\", \"feature_extractor.layer3.3.conv3.weight\", \"feature_extractor.layer3.3.bn3.weight\", \"feature_extractor.layer3.3.bn3.bias\", \"feature_extractor.layer3.3.bn3.running_mean\", \"feature_extractor.layer3.3.bn3.running_var\", \"feature_extractor.layer3.4.conv1.weight\", \"feature_extractor.layer3.4.bn1.weight\", \"feature_extractor.layer3.4.bn1.bias\", \"feature_extractor.layer3.4.bn1.running_mean\", \"feature_extractor.layer3.4.bn1.running_var\", \"feature_extractor.layer3.4.conv2.weight\", \"feature_extractor.layer3.4.bn2.weight\", \"feature_extractor.layer3.4.bn2.bias\", \"feature_extractor.layer3.4.bn2.running_mean\", \"feature_extractor.layer3.4.bn2.running_var\", \"feature_extractor.layer3.4.conv3.weight\", \"feature_extractor.layer3.4.bn3.weight\", \"feature_extractor.layer3.4.bn3.bias\", \"feature_extractor.layer3.4.bn3.running_mean\", \"feature_extractor.layer3.4.bn3.running_var\", \"feature_extractor.layer3.5.conv1.weight\", \"feature_extractor.layer3.5.bn1.weight\", \"feature_extractor.layer3.5.bn1.bias\", \"feature_extractor.layer3.5.bn1.running_mean\", \"feature_extractor.layer3.5.bn1.running_var\", \"feature_extractor.layer3.5.conv2.weight\", \"feature_extractor.layer3.5.bn2.weight\", \"feature_extractor.layer3.5.bn2.bias\", \"feature_extractor.layer3.5.bn2.running_mean\", \"feature_extractor.layer3.5.bn2.running_var\", \"feature_extractor.layer3.5.conv3.weight\", \"feature_extractor.layer3.5.bn3.weight\", \"feature_extractor.layer3.5.bn3.bias\", \"feature_extractor.layer3.5.bn3.running_mean\", \"feature_extractor.layer3.5.bn3.running_var\", \"feature_extractor.layer3.6.conv1.weight\", \"feature_extractor.layer3.6.bn1.weight\", \"feature_extractor.layer3.6.bn1.bias\", \"feature_extractor.layer3.6.bn1.running_mean\", \"feature_extractor.layer3.6.bn1.running_var\", \"feature_extractor.layer3.6.conv2.weight\", \"feature_extractor.layer3.6.bn2.weight\", \"feature_extractor.layer3.6.bn2.bias\", \"feature_extractor.layer3.6.bn2.running_mean\", \"feature_extractor.layer3.6.bn2.running_var\", \"feature_extractor.layer3.6.conv3.weight\", \"feature_extractor.layer3.6.bn3.weight\", \"feature_extractor.layer3.6.bn3.bias\", \"feature_extractor.layer3.6.bn3.running_mean\", \"feature_extractor.layer3.6.bn3.running_var\", \"feature_extractor.layer3.7.conv1.weight\", \"feature_extractor.layer3.7.bn1.weight\", \"feature_extractor.layer3.7.bn1.bias\", \"feature_extractor.layer3.7.bn1.running_mean\", \"feature_extractor.layer3.7.bn1.running_var\", \"feature_extractor.layer3.7.conv2.weight\", \"feature_extractor.layer3.7.bn2.weight\", \"feature_extractor.layer3.7.bn2.bias\", \"feature_extractor.layer3.7.bn2.running_mean\", \"feature_extractor.layer3.7.bn2.running_var\", \"feature_extractor.layer3.7.conv3.weight\", \"feature_extractor.layer3.7.bn3.weight\", \"feature_extractor.layer3.7.bn3.bias\", \"feature_extractor.layer3.7.bn3.running_mean\", \"feature_extractor.layer3.7.bn3.running_var\", \"feature_extractor.layer3.8.conv1.weight\", \"feature_extractor.layer3.8.bn1.weight\", \"feature_extractor.layer3.8.bn1.bias\", \"feature_extractor.layer3.8.bn1.running_mean\", \"feature_extractor.layer3.8.bn1.running_var\", \"feature_extractor.layer3.8.conv2.weight\", \"feature_extractor.layer3.8.bn2.weight\", \"feature_extractor.layer3.8.bn2.bias\", \"feature_extractor.layer3.8.bn2.running_mean\", \"feature_extractor.layer3.8.bn2.running_var\", \"feature_extractor.layer3.8.conv3.weight\", \"feature_extractor.layer3.8.bn3.weight\", \"feature_extractor.layer3.8.bn3.bias\", \"feature_extractor.layer3.8.bn3.running_mean\", \"feature_extractor.layer3.8.bn3.running_var\", \"feature_extractor.layer3.9.conv1.weight\", \"feature_extractor.layer3.9.bn1.weight\", \"feature_extractor.layer3.9.bn1.bias\", \"feature_extractor.layer3.9.bn1.running_mean\", \"feature_extractor.layer3.9.bn1.running_var\", \"feature_extractor.layer3.9.conv2.weight\", \"feature_extractor.layer3.9.bn2.weight\", \"feature_extractor.layer3.9.bn2.bias\", \"feature_extractor.layer3.9.bn2.running_mean\", \"feature_extractor.layer3.9.bn2.running_var\", \"feature_extractor.layer3.9.conv3.weight\", \"feature_extractor.layer3.9.bn3.weight\", \"feature_extractor.layer3.9.bn3.bias\", \"feature_extractor.layer3.9.bn3.running_mean\", \"feature_extractor.layer3.9.bn3.running_var\", \"feature_extractor.layer3.10.conv1.weight\", \"feature_extractor.layer3.10.bn1.weight\", \"feature_extractor.layer3.10.bn1.bias\", \"feature_extractor.layer3.10.bn1.running_mean\", \"feature_extractor.layer3.10.bn1.running_var\", \"feature_extractor.layer3.10.conv2.weight\", \"feature_extractor.layer3.10.bn2.weight\", \"feature_extractor.layer3.10.bn2.bias\", \"feature_extractor.layer3.10.bn2.running_mean\", \"feature_extractor.layer3.10.bn2.running_var\", \"feature_extractor.layer3.10.conv3.weight\", \"feature_extractor.layer3.10.bn3.weight\", \"feature_extractor.layer3.10.bn3.bias\", \"feature_extractor.layer3.10.bn3.running_mean\", \"feature_extractor.layer3.10.bn3.running_var\", \"feature_extractor.layer3.11.conv1.weight\", \"feature_extractor.layer3.11.bn1.weight\", \"feature_extractor.layer3.11.bn1.bias\", \"feature_extractor.layer3.11.bn1.running_mean\", \"feature_extractor.layer3.11.bn1.running_var\", \"feature_extractor.layer3.11.conv2.weight\", \"feature_extractor.layer3.11.bn2.weight\", \"feature_extractor.layer3.11.bn2.bias\", \"feature_extractor.layer3.11.bn2.running_mean\", \"feature_extractor.layer3.11.bn2.running_var\", \"feature_extractor.layer3.11.conv3.weight\", \"feature_extractor.layer3.11.bn3.weight\", \"feature_extractor.layer3.11.bn3.bias\", \"feature_extractor.layer3.11.bn3.running_mean\", \"feature_extractor.layer3.11.bn3.running_var\", \"feature_extractor.layer3.12.conv1.weight\", \"feature_extractor.layer3.12.bn1.weight\", \"feature_extractor.layer3.12.bn1.bias\", \"feature_extractor.layer3.12.bn1.running_mean\", \"feature_extractor.layer3.12.bn1.running_var\", \"feature_extractor.layer3.12.conv2.weight\", \"feature_extractor.layer3.12.bn2.weight\", \"feature_extractor.layer3.12.bn2.bias\", \"feature_extractor.layer3.12.bn2.running_mean\", \"feature_extractor.layer3.12.bn2.running_var\", \"feature_extractor.layer3.12.conv3.weight\", \"feature_extractor.layer3.12.bn3.weight\", \"feature_extractor.layer3.12.bn3.bias\", \"feature_extractor.layer3.12.bn3.running_mean\", \"feature_extractor.layer3.12.bn3.running_var\", \"feature_extractor.layer3.13.conv1.weight\", \"feature_extractor.layer3.13.bn1.weight\", \"feature_extractor.layer3.13.bn1.bias\", \"feature_extractor.layer3.13.bn1.running_mean\", \"feature_extractor.layer3.13.bn1.running_var\", \"feature_extractor.layer3.13.conv2.weight\", \"feature_extractor.layer3.13.bn2.weight\", \"feature_extractor.layer3.13.bn2.bias\", \"feature_extractor.layer3.13.bn2.running_mean\", \"feature_extractor.layer3.13.bn2.running_var\", \"feature_extractor.layer3.13.conv3.weight\", \"feature_extractor.layer3.13.bn3.weight\", \"feature_extractor.layer3.13.bn3.bias\", \"feature_extractor.layer3.13.bn3.running_mean\", \"feature_extractor.layer3.13.bn3.running_var\", \"feature_extractor.layer3.14.conv1.weight\", \"feature_extractor.layer3.14.bn1.weight\", \"feature_extractor.layer3.14.bn1.bias\", \"feature_extractor.layer3.14.bn1.running_mean\", \"feature_extractor.layer3.14.bn1.running_var\", \"feature_extractor.layer3.14.conv2.weight\", \"feature_extractor.layer3.14.bn2.weight\", \"feature_extractor.layer3.14.bn2.bias\", \"feature_extractor.layer3.14.bn2.running_mean\", \"feature_extractor.layer3.14.bn2.running_var\", \"feature_extractor.layer3.14.conv3.weight\", \"feature_extractor.layer3.14.bn3.weight\", \"feature_extractor.layer3.14.bn3.bias\", \"feature_extractor.layer3.14.bn3.running_mean\", \"feature_extractor.layer3.14.bn3.running_var\", \"feature_extractor.layer3.15.conv1.weight\", \"feature_extractor.layer3.15.bn1.weight\", \"feature_extractor.layer3.15.bn1.bias\", \"feature_extractor.layer3.15.bn1.running_mean\", \"feature_extractor.layer3.15.bn1.running_var\", \"feature_extractor.layer3.15.conv2.weight\", \"feature_extractor.layer3.15.bn2.weight\", \"feature_extractor.layer3.15.bn2.bias\", \"feature_extractor.layer3.15.bn2.running_mean\", \"feature_extractor.layer3.15.bn2.running_var\", \"feature_extractor.layer3.15.conv3.weight\", \"feature_extractor.layer3.15.bn3.weight\", \"feature_extractor.layer3.15.bn3.bias\", \"feature_extractor.layer3.15.bn3.running_mean\", \"feature_extractor.layer3.15.bn3.running_var\", \"feature_extractor.layer3.16.conv1.weight\", \"feature_extractor.layer3.16.bn1.weight\", \"feature_extractor.layer3.16.bn1.bias\", \"feature_extractor.layer3.16.bn1.running_mean\", \"feature_extractor.layer3.16.bn1.running_var\", \"feature_extractor.layer3.16.conv2.weight\", \"feature_extractor.layer3.16.bn2.weight\", \"feature_extractor.layer3.16.bn2.bias\", \"feature_extractor.layer3.16.bn2.running_mean\", \"feature_extractor.layer3.16.bn2.running_var\", \"feature_extractor.layer3.16.conv3.weight\", \"feature_extractor.layer3.16.bn3.weight\", \"feature_extractor.layer3.16.bn3.bias\", \"feature_extractor.layer3.16.bn3.running_mean\", \"feature_extractor.layer3.16.bn3.running_var\", \"feature_extractor.layer3.17.conv1.weight\", \"feature_extractor.layer3.17.bn1.weight\", \"feature_extractor.layer3.17.bn1.bias\", \"feature_extractor.layer3.17.bn1.running_mean\", \"feature_extractor.layer3.17.bn1.running_var\", \"feature_extractor.layer3.17.conv2.weight\", \"feature_extractor.layer3.17.bn2.weight\", \"feature_extractor.layer3.17.bn2.bias\", \"feature_extractor.layer3.17.bn2.running_mean\", \"feature_extractor.layer3.17.bn2.running_var\", \"feature_extractor.layer3.17.conv3.weight\", \"feature_extractor.layer3.17.bn3.weight\", \"feature_extractor.layer3.17.bn3.bias\", \"feature_extractor.layer3.17.bn3.running_mean\", \"feature_extractor.layer3.17.bn3.running_var\", \"feature_extractor.layer3.18.conv1.weight\", \"feature_extractor.layer3.18.bn1.weight\", \"feature_extractor.layer3.18.bn1.bias\", \"feature_extractor.layer3.18.bn1.running_mean\", \"feature_extractor.layer3.18.bn1.running_var\", \"feature_extractor.layer3.18.conv2.weight\", \"feature_extractor.layer3.18.bn2.weight\", \"feature_extractor.layer3.18.bn2.bias\", \"feature_extractor.layer3.18.bn2.running_mean\", \"feature_extractor.layer3.18.bn2.running_var\", \"feature_extractor.layer3.18.conv3.weight\", \"feature_extractor.layer3.18.bn3.weight\", \"feature_extractor.layer3.18.bn3.bias\", \"feature_extractor.layer3.18.bn3.running_mean\", \"feature_extractor.layer3.18.bn3.running_var\", \"feature_extractor.layer3.19.conv1.weight\", \"feature_extractor.layer3.19.bn1.weight\", \"feature_extractor.layer3.19.bn1.bias\", \"feature_extractor.layer3.19.bn1.running_mean\", \"feature_extractor.layer3.19.bn1.running_var\", \"feature_extractor.layer3.19.conv2.weight\", \"feature_extractor.layer3.19.bn2.weight\", \"feature_extractor.layer3.19.bn2.bias\", \"feature_extractor.layer3.19.bn2.running_mean\", \"feature_extractor.layer3.19.bn2.running_var\", \"feature_extractor.layer3.19.conv3.weight\", \"feature_extractor.layer3.19.bn3.weight\", \"feature_extractor.layer3.19.bn3.bias\", \"feature_extractor.layer3.19.bn3.running_mean\", \"feature_extractor.layer3.19.bn3.running_var\", \"feature_extractor.layer3.20.conv1.weight\", \"feature_extractor.layer3.20.bn1.weight\", \"feature_extractor.layer3.20.bn1.bias\", \"feature_extractor.layer3.20.bn1.running_mean\", \"feature_extractor.layer3.20.bn1.running_var\", \"feature_extractor.layer3.20.conv2.weight\", \"feature_extractor.layer3.20.bn2.weight\", \"feature_extractor.layer3.20.bn2.bias\", \"feature_extractor.layer3.20.bn2.running_mean\", \"feature_extractor.layer3.20.bn2.running_var\", \"feature_extractor.layer3.20.conv3.weight\", \"feature_extractor.layer3.20.bn3.weight\", \"feature_extractor.layer3.20.bn3.bias\", \"feature_extractor.layer3.20.bn3.running_mean\", \"feature_extractor.layer3.20.bn3.running_var\", \"feature_extractor.layer3.21.conv1.weight\", \"feature_extractor.layer3.21.bn1.weight\", \"feature_extractor.layer3.21.bn1.bias\", \"feature_extractor.layer3.21.bn1.running_mean\", \"feature_extractor.layer3.21.bn1.running_var\", \"feature_extractor.layer3.21.conv2.weight\", \"feature_extractor.layer3.21.bn2.weight\", \"feature_extractor.layer3.21.bn2.bias\", \"feature_extractor.layer3.21.bn2.running_mean\", \"feature_extractor.layer3.21.bn2.running_var\", \"feature_extractor.layer3.21.conv3.weight\", \"feature_extractor.layer3.21.bn3.weight\", \"feature_extractor.layer3.21.bn3.bias\", \"feature_extractor.layer3.21.bn3.running_mean\", \"feature_extractor.layer3.21.bn3.running_var\", \"feature_extractor.layer3.22.conv1.weight\", \"feature_extractor.layer3.22.bn1.weight\", \"feature_extractor.layer3.22.bn1.bias\", \"feature_extractor.layer3.22.bn1.running_mean\", \"feature_extractor.layer3.22.bn1.running_var\", \"feature_extractor.layer3.22.conv2.weight\", \"feature_extractor.layer3.22.bn2.weight\", \"feature_extractor.layer3.22.bn2.bias\", \"feature_extractor.layer3.22.bn2.running_mean\", \"feature_extractor.layer3.22.bn2.running_var\", \"feature_extractor.layer3.22.conv3.weight\", \"feature_extractor.layer3.22.bn3.weight\", \"feature_extractor.layer3.22.bn3.bias\", \"feature_extractor.layer3.22.bn3.running_mean\", \"feature_extractor.layer3.22.bn3.running_var\", \"feature_extractor.layer3.23.conv1.weight\", \"feature_extractor.layer3.23.bn1.weight\", \"feature_extractor.layer3.23.bn1.bias\", \"feature_extractor.layer3.23.bn1.running_mean\", \"feature_extractor.layer3.23.bn1.running_var\", \"feature_extractor.layer3.23.conv2.weight\", \"feature_extractor.layer3.23.bn2.weight\", \"feature_extractor.layer3.23.bn2.bias\", \"feature_extractor.layer3.23.bn2.running_mean\", \"feature_extractor.layer3.23.bn2.running_var\", \"feature_extractor.layer3.23.conv3.weight\", \"feature_extractor.layer3.23.bn3.weight\", \"feature_extractor.layer3.23.bn3.bias\", \"feature_extractor.layer3.23.bn3.running_mean\", \"feature_extractor.layer3.23.bn3.running_var\", \"feature_extractor.layer3.24.conv1.weight\", \"feature_extractor.layer3.24.bn1.weight\", \"feature_extractor.layer3.24.bn1.bias\", \"feature_extractor.layer3.24.bn1.running_mean\", \"feature_extractor.layer3.24.bn1.running_var\", \"feature_extractor.layer3.24.conv2.weight\", \"feature_extractor.layer3.24.bn2.weight\", \"feature_extractor.layer3.24.bn2.bias\", \"feature_extractor.layer3.24.bn2.running_mean\", \"feature_extractor.layer3.24.bn2.running_var\", \"feature_extractor.layer3.24.conv3.weight\", \"feature_extractor.layer3.24.bn3.weight\", \"feature_extractor.layer3.24.bn3.bias\", \"feature_extractor.layer3.24.bn3.running_mean\", \"feature_extractor.layer3.24.bn3.running_var\", \"feature_extractor.layer3.25.conv1.weight\", \"feature_extractor.layer3.25.bn1.weight\", \"feature_extractor.layer3.25.bn1.bias\", \"feature_extractor.layer3.25.bn1.running_mean\", \"feature_extractor.layer3.25.bn1.running_var\", \"feature_extractor.layer3.25.conv2.weight\", \"feature_extractor.layer3.25.bn2.weight\", \"feature_extractor.layer3.25.bn2.bias\", \"feature_extractor.layer3.25.bn2.running_mean\", \"feature_extractor.layer3.25.bn2.running_var\", \"feature_extractor.layer3.25.conv3.weight\", \"feature_extractor.layer3.25.bn3.weight\", \"feature_extractor.layer3.25.bn3.bias\", \"feature_extractor.layer3.25.bn3.running_mean\", \"feature_extractor.layer3.25.bn3.running_var\", \"feature_extractor.layer3.26.conv1.weight\", \"feature_extractor.layer3.26.bn1.weight\", \"feature_extractor.layer3.26.bn1.bias\", \"feature_extractor.layer3.26.bn1.running_mean\", \"feature_extractor.layer3.26.bn1.running_var\", \"feature_extractor.layer3.26.conv2.weight\", \"feature_extractor.layer3.26.bn2.weight\", \"feature_extractor.layer3.26.bn2.bias\", \"feature_extractor.layer3.26.bn2.running_mean\", \"feature_extractor.layer3.26.bn2.running_var\", \"feature_extractor.layer3.26.conv3.weight\", \"feature_extractor.layer3.26.bn3.weight\", \"feature_extractor.layer3.26.bn3.bias\", \"feature_extractor.layer3.26.bn3.running_mean\", \"feature_extractor.layer3.26.bn3.running_var\", \"feature_extractor.layer3.27.conv1.weight\", \"feature_extractor.layer3.27.bn1.weight\", \"feature_extractor.layer3.27.bn1.bias\", \"feature_extractor.layer3.27.bn1.running_mean\", \"feature_extractor.layer3.27.bn1.running_var\", \"feature_extractor.layer3.27.conv2.weight\", \"feature_extractor.layer3.27.bn2.weight\", \"feature_extractor.layer3.27.bn2.bias\", \"feature_extractor.layer3.27.bn2.running_mean\", \"feature_extractor.layer3.27.bn2.running_var\", \"feature_extractor.layer3.27.conv3.weight\", \"feature_extractor.layer3.27.bn3.weight\", \"feature_extractor.layer3.27.bn3.bias\", \"feature_extractor.layer3.27.bn3.running_mean\", \"feature_extractor.layer3.27.bn3.running_var\", \"feature_extractor.layer3.28.conv1.weight\", \"feature_extractor.layer3.28.bn1.weight\", \"feature_extractor.layer3.28.bn1.bias\", \"feature_extractor.layer3.28.bn1.running_mean\", \"feature_extractor.layer3.28.bn1.running_var\", \"feature_extractor.layer3.28.conv2.weight\", \"feature_extractor.layer3.28.bn2.weight\", \"feature_extractor.layer3.28.bn2.bias\", \"feature_extractor.layer3.28.bn2.running_mean\", \"feature_extractor.layer3.28.bn2.running_var\", \"feature_extractor.layer3.28.conv3.weight\", \"feature_extractor.layer3.28.bn3.weight\", \"feature_extractor.layer3.28.bn3.bias\", \"feature_extractor.layer3.28.bn3.running_mean\", \"feature_extractor.layer3.28.bn3.running_var\", \"feature_extractor.layer3.29.conv1.weight\", \"feature_extractor.layer3.29.bn1.weight\", \"feature_extractor.layer3.29.bn1.bias\", \"feature_extractor.layer3.29.bn1.running_mean\", \"feature_extractor.layer3.29.bn1.running_var\", \"feature_extractor.layer3.29.conv2.weight\", \"feature_extractor.layer3.29.bn2.weight\", \"feature_extractor.layer3.29.bn2.bias\", \"feature_extractor.layer3.29.bn2.running_mean\", \"feature_extractor.layer3.29.bn2.running_var\", \"feature_extractor.layer3.29.conv3.weight\", \"feature_extractor.layer3.29.bn3.weight\", \"feature_extractor.layer3.29.bn3.bias\", \"feature_extractor.layer3.29.bn3.running_mean\", \"feature_extractor.layer3.29.bn3.running_var\", \"feature_extractor.layer3.30.conv1.weight\", \"feature_extractor.layer3.30.bn1.weight\", \"feature_extractor.layer3.30.bn1.bias\", \"feature_extractor.layer3.30.bn1.running_mean\", \"feature_extractor.layer3.30.bn1.running_var\", \"feature_extractor.layer3.30.conv2.weight\", \"feature_extractor.layer3.30.bn2.weight\", \"feature_extractor.layer3.30.bn2.bias\", \"feature_extractor.layer3.30.bn2.running_mean\", \"feature_extractor.layer3.30.bn2.running_var\", \"feature_extractor.layer3.30.conv3.weight\", \"feature_extractor.layer3.30.bn3.weight\", \"feature_extractor.layer3.30.bn3.bias\", \"feature_extractor.layer3.30.bn3.running_mean\", \"feature_extractor.layer3.30.bn3.running_var\", \"feature_extractor.layer3.31.conv1.weight\", \"feature_extractor.layer3.31.bn1.weight\", \"feature_extractor.layer3.31.bn1.bias\", \"feature_extractor.layer3.31.bn1.running_mean\", \"feature_extractor.layer3.31.bn1.running_var\", \"feature_extractor.layer3.31.conv2.weight\", \"feature_extractor.layer3.31.bn2.weight\", \"feature_extractor.layer3.31.bn2.bias\", \"feature_extractor.layer3.31.bn2.running_mean\", \"feature_extractor.layer3.31.bn2.running_var\", \"feature_extractor.layer3.31.conv3.weight\", \"feature_extractor.layer3.31.bn3.weight\", \"feature_extractor.layer3.31.bn3.bias\", \"feature_extractor.layer3.31.bn3.running_mean\", \"feature_extractor.layer3.31.bn3.running_var\", \"feature_extractor.layer3.32.conv1.weight\", \"feature_extractor.layer3.32.bn1.weight\", \"feature_extractor.layer3.32.bn1.bias\", \"feature_extractor.layer3.32.bn1.running_mean\", \"feature_extractor.layer3.32.bn1.running_var\", \"feature_extractor.layer3.32.conv2.weight\", \"feature_extractor.layer3.32.bn2.weight\", \"feature_extractor.layer3.32.bn2.bias\", \"feature_extractor.layer3.32.bn2.running_mean\", \"feature_extractor.layer3.32.bn2.running_var\", \"feature_extractor.layer3.32.conv3.weight\", \"feature_extractor.layer3.32.bn3.weight\", \"feature_extractor.layer3.32.bn3.bias\", \"feature_extractor.layer3.32.bn3.running_mean\", \"feature_extractor.layer3.32.bn3.running_var\", \"feature_extractor.layer3.33.conv1.weight\", \"feature_extractor.layer3.33.bn1.weight\", \"feature_extractor.layer3.33.bn1.bias\", \"feature_extractor.layer3.33.bn1.running_mean\", \"feature_extractor.layer3.33.bn1.running_var\", \"feature_extractor.layer3.33.conv2.weight\", \"feature_extractor.layer3.33.bn2.weight\", \"feature_extractor.layer3.33.bn2.bias\", \"feature_extractor.layer3.33.bn2.running_mean\", \"feature_extractor.layer3.33.bn2.running_var\", \"feature_extractor.layer3.33.conv3.weight\", \"feature_extractor.layer3.33.bn3.weight\", \"feature_extractor.layer3.33.bn3.bias\", \"feature_extractor.layer3.33.bn3.running_mean\", \"feature_extractor.layer3.33.bn3.running_var\", \"feature_extractor.layer3.34.conv1.weight\", \"feature_extractor.layer3.34.bn1.weight\", \"feature_extractor.layer3.34.bn1.bias\", \"feature_extractor.layer3.34.bn1.running_mean\", \"feature_extractor.layer3.34.bn1.running_var\", \"feature_extractor.layer3.34.conv2.weight\", \"feature_extractor.layer3.34.bn2.weight\", \"feature_extractor.layer3.34.bn2.bias\", \"feature_extractor.layer3.34.bn2.running_mean\", \"feature_extractor.layer3.34.bn2.running_var\", \"feature_extractor.layer3.34.conv3.weight\", \"feature_extractor.layer3.34.bn3.weight\", \"feature_extractor.layer3.34.bn3.bias\", \"feature_extractor.layer3.34.bn3.running_mean\", \"feature_extractor.layer3.34.bn3.running_var\", \"feature_extractor.layer3.35.conv1.weight\", \"feature_extractor.layer3.35.bn1.weight\", \"feature_extractor.layer3.35.bn1.bias\", \"feature_extractor.layer3.35.bn1.running_mean\", \"feature_extractor.layer3.35.bn1.running_var\", \"feature_extractor.layer3.35.conv2.weight\", \"feature_extractor.layer3.35.bn2.weight\", \"feature_extractor.layer3.35.bn2.bias\", \"feature_extractor.layer3.35.bn2.running_mean\", \"feature_extractor.layer3.35.bn2.running_var\", \"feature_extractor.layer3.35.conv3.weight\", \"feature_extractor.layer3.35.bn3.weight\", \"feature_extractor.layer3.35.bn3.bias\", \"feature_extractor.layer3.35.bn3.running_mean\", \"feature_extractor.layer3.35.bn3.running_var\", \"feature_extractor.layer4.0.conv3.weight\", \"feature_extractor.layer4.0.bn3.weight\", \"feature_extractor.layer4.0.bn3.bias\", \"feature_extractor.layer4.0.bn3.running_mean\", \"feature_extractor.layer4.0.bn3.running_var\", \"feature_extractor.layer4.1.conv3.weight\", \"feature_extractor.layer4.1.bn3.weight\", \"feature_extractor.layer4.1.bn3.bias\", \"feature_extractor.layer4.1.bn3.running_mean\", \"feature_extractor.layer4.1.bn3.running_var\", \"feature_extractor.layer4.2.conv1.weight\", \"feature_extractor.layer4.2.bn1.weight\", \"feature_extractor.layer4.2.bn1.bias\", \"feature_extractor.layer4.2.bn1.running_mean\", \"feature_extractor.layer4.2.bn1.running_var\", \"feature_extractor.layer4.2.conv2.weight\", \"feature_extractor.layer4.2.bn2.weight\", \"feature_extractor.layer4.2.bn2.bias\", \"feature_extractor.layer4.2.bn2.running_mean\", \"feature_extractor.layer4.2.bn2.running_var\", \"feature_extractor.layer4.2.conv3.weight\", \"feature_extractor.layer4.2.bn3.weight\", \"feature_extractor.layer4.2.bn3.bias\", \"feature_extractor.layer4.2.bn3.running_mean\", \"feature_extractor.layer4.2.bn3.running_var\". \n\tsize mismatch for feature_extractor.layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for feature_extractor.layer1.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for feature_extractor.layer2.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for feature_extractor.layer2.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for feature_extractor.layer3.0.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 512, 1, 1]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for feature_extractor.layer3.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for feature_extractor.layer4.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for feature_extractor.layer4.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for feature_extractor.fc.weight: copying a param with shape torch.Size([1000, 512]) from checkpoint, the shape in current model is torch.Size([1000, 2048]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m emotion_model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m age_model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m emotion_model\u001b[39m.\u001b[39;49mload_state_dict(emotion_wt)\n\u001b[1;32m     19\u001b[0m gender_model\u001b[39m.\u001b[39mload_state_dict(gender_wt)\n\u001b[1;32m     20\u001b[0m age_model\u001b[39m.\u001b[39mload_state_dict(age_wt)\n",
      "File \u001b[0;32m~/anaconda3/envs/face/lib/python3.8/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EmotionModel:\n\tMissing key(s) in state_dict: \"feature_extractor.layer1.0.conv3.weight\", \"feature_extractor.layer1.0.bn3.weight\", \"feature_extractor.layer1.0.bn3.bias\", \"feature_extractor.layer1.0.bn3.running_mean\", \"feature_extractor.layer1.0.bn3.running_var\", \"feature_extractor.layer1.0.downsample.0.weight\", \"feature_extractor.layer1.0.downsample.1.weight\", \"feature_extractor.layer1.0.downsample.1.bias\", \"feature_extractor.layer1.0.downsample.1.running_mean\", \"feature_extractor.layer1.0.downsample.1.running_var\", \"feature_extractor.layer1.1.conv3.weight\", \"feature_extractor.layer1.1.bn3.weight\", \"feature_extractor.layer1.1.bn3.bias\", \"feature_extractor.layer1.1.bn3.running_mean\", \"feature_extractor.layer1.1.bn3.running_var\", \"feature_extractor.layer1.2.conv1.weight\", \"feature_extractor.layer1.2.bn1.weight\", \"feature_extractor.layer1.2.bn1.bias\", \"feature_extractor.layer1.2.bn1.running_mean\", \"feature_extractor.layer1.2.bn1.running_var\", \"feature_extractor.layer1.2.conv2.weight\", \"feature_extractor.layer1.2.bn2.weight\", \"feature_extractor.layer1.2.bn2.bias\", \"feature_extractor.layer1.2.bn2.running_mean\", \"feature_extractor.layer1.2.bn2.running_var\", \"feature_extractor.layer1.2.conv3.weight\", \"feature_extractor.layer1.2.bn3.weight\", \"feature_extractor.layer1.2.bn3.bias\", \"feature_extractor.layer1.2.bn3.running_mean\", \"feature_extractor.layer1.2.bn3.running_var\", \"feature_extractor.layer2.0.conv3.weight\", \"feature_extractor.layer2.0.bn3.weight\", \"feature_extractor.layer2.0.bn3.bias\", \"feature_extractor.layer2.0.bn3.running_mean\", \"feature_extractor.layer2.0.bn3.running_var\", \"feature_extractor.layer2.1.conv3.weight\", \"feature_extractor.layer2.1.bn3.weight\", \"feature_extractor.layer2.1.bn3.bias\", \"feature_extractor.layer2.1.bn3.running_mean\", \"feature_extractor.layer2.1.bn3.running_var\", \"feature_extractor.layer2.2.conv1.weight\", \"feature_extractor.layer2.2.bn1.weight\", \"feature_extractor.layer2.2.bn1.bias\", \"feature_extractor.layer2.2.bn1.running_mean\", \"feature_extractor.layer2.2.bn1.running_var\", \"feature_extractor.layer2.2.conv2.weight\", \"feature_extractor.layer2.2.bn2.weight\", \"feature_extractor.layer2.2.bn2.bias\", \"feature_extractor.layer2.2.bn2.running_mean\", \"feature_extractor.layer2.2.bn2.running_var\", \"feature_extractor.layer2.2.conv3.weight\", \"feature_extractor.layer2.2.bn3.weight\", \"feature_extractor.layer2.2.bn3.bias\", \"feature_extractor.layer2.2.bn3.running_mean\", \"feature_extractor.layer2.2.bn3.running_var\", \"feature_extractor.layer2.3.conv1.weight\", \"feature_extractor.layer2.3.bn1.weight\", \"feature_extractor.layer2.3.bn1.bias\", \"feature_extractor.layer2.3.bn1.running_mean\", \"feature_extractor.layer2.3.bn1.running_var\", \"feature_extractor.layer2.3.conv2.weight\", \"feature_extractor.layer2.3.bn2.weight\", \"feature_extractor.layer2.3.bn2.bias\", \"feature_extractor.layer2.3.bn2.running_mean\", \"feature_extractor.layer2.3.bn2.running_var\", \"feature_extractor.layer2.3.conv3.weight\", \"feature_extractor.layer2.3.bn3.weight\", \"feature_extractor.layer2.3.bn3.bias\", \"feature_extractor.layer2.3.bn3.running_mean\", \"feature_extractor.layer2.3.bn3.running_var\", \"feature_extractor.layer2.4.conv1.weight\", \"feature_extractor.layer2.4.bn1.weight\", \"feature_extractor.layer2.4.bn1.bias\", \"feature_extractor.layer2.4.bn1.running_mean\", \"feature_extractor.layer2.4.bn1.running_var\", \"feature_extractor.layer2.4.conv2.weight\", \"feature_extractor.layer2.4.bn2.weight\", \"feature_extractor.layer2.4.bn2.bias\", \"feature_extractor.layer2.4.bn2.running_mean\", \"feature_extractor.layer2.4.bn2.running_var\", \"feature_extractor.layer2.4.conv3.weight\", \"feature_extractor.layer2.4.bn3.weight\", \"feature_extractor.layer2.4.bn3.bias\", \"feature_extractor.layer2.4.bn3.running_mean\", \"feature_extractor.layer2.4.bn3.running_var\", \"feature_extractor.layer2.5.conv1.weight\", \"feature_extractor.layer2.5.bn1.weight\", \"feature_extractor.layer2.5.bn1.bias\", \"feature_extractor.layer2.5.bn1.running_mean\", \"feature_extractor.layer2.5.bn1.running_var\", \"feature_extractor.layer2.5.conv2.weight\", \"feature_extractor.layer2.5.bn2.weight\", \"feature_extractor.layer2.5.bn2.bias\", \"feature_extractor.layer2.5.bn2.running_mean\", \"feature_extractor.layer2.5.bn2.running_var\", \"feature_extractor.layer2.5.conv3.weight\", \"feature_extractor.layer2.5.bn3.weight\", \"feature_extractor.layer2.5.bn3.bias\", \"feature_extractor.layer2.5.bn3.running_mean\", \"feature_extractor.layer2.5.bn3.running_var\", \"feature_extractor.layer2.6.conv1.weight\", \"feature_extractor.layer2.6.bn1.weight\", \"feature_extractor.layer2.6.bn1.bias\", \"feature_extractor.layer2.6.bn1.running_mean\", \"feature_extractor.layer2.6.bn1.running_var\", \"feature_extractor.layer2.6.conv2.weight\", \"feature_extractor.layer2.6.bn2.weight\", \"feature_extractor.layer2.6.bn2.bias\", \"feature_extractor.layer2.6.bn2.running_mean\", \"feature_extractor.layer2.6.bn2.running_var\", \"feature_extractor.layer2.6.conv3.weight\", \"feature_extractor.layer2.6.bn3.weight\", \"feature_extractor.layer2.6.bn3.bias\", \"feature_extractor.layer2.6.bn3.running_mean\", \"feature_extractor.layer2.6.bn3.running_var\", \"feature_extractor.layer2.7.conv1.weight\", \"feature_extractor.layer2.7.bn1.weight\", \"feature_extractor.layer2.7.bn1.bias\", \"feature_extractor.layer2.7.bn1.running_mean\", \"feature_extractor.layer2.7.bn1.running_var\", \"feature_extractor.layer2.7.conv2.weight\", \"feature_extractor.layer2.7.bn2.weight\", \"feature_extractor.layer2.7.bn2.bias\", \"feature_extractor.layer2.7.bn2.running_mean\", \"feature_extractor.layer2.7.bn2.running_var\", \"feature_extractor.layer2.7.conv3.weight\", \"feature_extractor.layer2.7.bn3.weight\", \"feature_extractor.layer2.7.bn3.bias\", \"feature_extractor.layer2.7.bn3.running_mean\", \"feature_extractor.layer2.7.bn3.running_var\", \"feature_extractor.layer3.0.conv3.weight\", \"feature_extractor.layer3.0.bn3.weight\", \"feature_extractor.layer3.0.bn3.bias\", \"feature_extractor.layer3.0.bn3.running_mean\", \"feature_extractor.layer3.0.bn3.running_var\", \"feature_extractor.layer3.1.conv3.weight\", \"feature_extractor.layer3.1.bn3.weight\", \"feature_extractor.layer3.1.bn3.bias\", \"feature_extractor.layer3.1.bn3.running_mean\", \"feature_extractor.layer3.1.bn3.running_var\", \"feature_extractor.layer3.2.conv1.weight\", \"feature_extractor.layer3.2.bn1.weight\", \"feature_extractor.layer3.2.bn1.bias\", \"feature_extractor.layer3.2.bn1.running_mean\", \"feature_extractor.layer3.2.bn1.running_var\", \"feature_extractor.layer3.2.conv2.weight\", \"feature_extractor.layer3.2.bn2.weight\", \"feature_extractor.layer3.2.bn2.bias\", \"feature_extractor.layer3.2.bn2.running_mean\", \"feature_extractor.layer3.2.bn2.running_var\", \"feature_extractor.layer3.2.conv3.weight\", \"feature_extractor.layer3.2.bn3.weight\", \"feature_extractor.layer3.2.bn3.bias\", \"feature_extractor.layer3.2.bn3.running_mean\", \"feature_extractor.layer3.2.bn3.running_var\", \"feature_extractor.layer3.3.conv1.weight\", \"feature_extractor.layer3.3.bn1.weight\", \"feature_extractor.layer3.3.bn1.bias\", \"feature_extractor.layer3.3.bn1.running_mean\", \"feature_extractor.layer3.3.bn1.running_var\", \"feature_extractor.layer3.3.conv2.weight\", \"feature_extractor.layer3.3.bn2.weight\", \"feature_extractor.layer3.3.bn2.bias\", \"feature_extractor.layer3.3.bn2.running_mean\", \"feature_extractor.layer3.3.bn2.running_var\", \"feature_extractor.layer3.3.conv3.weight\", \"feature_extractor.layer3.3.bn3.weight\", \"feature_extractor.layer3.3.bn3.bias\", \"feature_extractor.layer3.3.bn3.running_mean\", \"feature_extractor.layer3.3.bn3.running_var\", \"feature_extractor.layer3.4.conv1.weight\", \"feature_extractor.layer3.4.bn1.weight\", \"feature_extractor.layer3.4.bn1.bias\", \"feature_extractor.layer3.4.bn1.running_mean\", \"feature_extractor.layer3.4.bn1.running_var\", \"feature_extractor.layer3.4.conv2.weight\", \"feature_extractor.layer3.4.bn2.weight\", \"feature_extractor.layer3.4.bn2.bias\", \"feature_extractor.layer3.4.bn2.running_mean\", \"feature_extractor.layer3.4.bn2.running_var\", \"feature_extractor.layer3.4.conv3.weight\", \"feature_extractor.layer3.4.bn3.weight\", \"feature_extractor.layer3.4.bn3.bias\", \"feature_extractor.layer3.4.bn3.running_mean\", \"feature_extractor.layer3.4.bn3.running_var\", \"feature_extractor.layer3.5.conv1.weight\", \"feature_extractor.layer3.5.bn1.weight\", \"feature_extractor.layer3.5.bn1.bias\", \"feature_extractor.layer3.5.bn1.running_mean\", \"feature_extractor.layer3.5.bn1.running_var\", \"feature_extractor.layer3.5.conv2.weight\", \"feature_extractor.layer3.5.bn2.weight\", \"feature_extractor.layer3.5.bn2.bias\", \"feature_extractor.layer3.5.bn2.running_mean\", \"feature_extractor.layer3.5.bn2.running_var\", \"feature_extractor.layer3.5.conv3.weight\", \"feature_extractor.layer3.5.bn3.weight\", \"feature_extractor.layer3.5.bn3.bias\", \"feature_extractor.layer3.5.bn3.running_mean\", \"feature_extractor.layer3.5.bn3.running_var\", \"feature_extractor.layer3.6.conv1.weight\", \"feature_extractor.layer3.6.bn1.weight\", \"feature_extractor.layer3.6.bn1.bias\", \"feature_extractor.layer3.6.bn1.running_mean\", \"feature_extractor.layer3.6.bn1.running_var\", \"feature_extractor.layer3.6.conv2.weight\", \"feature_extractor.layer3.6.bn2.weight\", \"feature_extractor.layer3.6.bn2.bias\", \"feature_extractor.layer3.6.bn2.running_mean\", \"feature_extractor.layer3.6.bn2.running_var\", \"feature_extractor.layer3.6.conv3.weight\", \"feature_extractor.layer3.6.bn3.weight\", \"feature_extractor.layer3.6.bn3.bias\", \"feature_extractor.layer3.6.bn3.running_mean\", \"feature_extractor.layer3.6.bn3.running_var\", \"feature_extractor.layer3.7.conv1.weight\", \"feature_extractor.layer3.7.bn1.weight\", \"feature_extractor.layer3.7.bn1.bias\", \"feature_extractor.layer3.7.bn1.running_mean\", \"feature_extractor.layer3.7.bn1.running_var\", \"feature_extractor.layer3.7.conv2.weight\", \"feature_extractor.layer3.7.bn2.weight\", \"feature_extractor.layer3.7.bn2.bias\", \"feature_extractor.layer3.7.bn2.running_mean\", \"feature_extractor.layer3.7.bn2.running_var\", \"feature_extractor.layer3.7.conv3.weight\", \"feature_extractor.layer3.7.bn3.weight\", \"feature_extractor.layer3.7.bn3.bias\", \"feature_extractor.layer3.7.bn3.running_mean\", \"feature_extractor.layer3.7.bn3.running_var\", \"feature_extractor.layer3.8.conv1.weight\", \"feature_extractor.layer3.8.bn1.weight\", \"feature_extractor.layer3.8.bn1.bias\", \"feature_extractor.layer3.8.bn1.running_mean\", \"feature_extractor.layer3.8.bn1.running_var\", \"feature_extractor.layer3.8.conv2.weight\", \"feature_extractor.layer3.8.bn2.weight\", \"feature_extractor.layer3.8.bn2.bias\", \"feature_extractor.layer3.8.bn2.running_mean\", \"feature_extractor.layer3.8.bn2.running_var\", \"feature_extractor.layer3.8.conv3.weight\", \"feature_extractor.layer3.8.bn3.weight\", \"feature_extractor.layer3.8.bn3.bias\", \"feature_extractor.layer3.8.bn3.running_mean\", \"feature_extractor.layer3.8.bn3.running_var\", \"feature_extractor.layer3.9.conv1.weight\", \"feature_extractor.layer3.9.bn1.weight\", \"feature_extractor.layer3.9.bn1.bias\", \"feature_extractor.layer3.9.bn1.running_mean\", \"feature_extractor.layer3.9.bn1.running_var\", \"feature_extractor.layer3.9.conv2.weight\", \"feature_extractor.layer3.9.bn2.weight\", \"feature_extractor.layer3.9.bn2.bias\", \"feature_extractor.layer3.9.bn2.running_mean\", \"feature_extractor.layer3.9.bn2.running_var\", \"feature_extractor.layer3.9.conv3.weight\", \"feature_extractor.layer3.9.bn3.weight\", \"feature_extractor.layer3.9.bn3.bias\", \"feature_extractor.layer3.9.bn3.running_mean\", \"feature_extractor.layer3.9.bn3.running_var\", \"feature_extractor.layer3.10.conv1.weight\", \"feature_extractor.layer3.10.bn1.weight\", \"feature_extractor.layer3.10.bn1.bias\", \"feature_extractor.layer3.10.bn1.running_mean\", \"feature_extractor.layer3.10.bn1.running_var\", \"feature_extractor.layer3.10.conv2.weight\", \"feature_extractor.layer3.10.bn2.weight\", \"feature_extractor.layer3.10.bn2.bias\", \"feature_extractor.layer3.10.bn2.running_mean\", \"feature_extractor.layer3.10.bn2.running_var\", \"feature_extractor.layer3.10.conv3.weight\", \"feature_extractor.layer3.10.bn3.weight\", \"feature_extractor.layer3.10.bn3.bias\", \"feature_extractor.layer3.10.bn3.running_mean\", \"feature_extractor.layer3.10.bn3.running_var\", \"feature_extractor.layer3.11.conv1.weight\", \"feature_extractor.layer3.11.bn1.weight\", \"feature_extractor.layer3.11.bn1.bias\", \"feature_extractor.layer3.11.bn1.running_mean\", \"feature_extractor.layer3.11.bn1.running_var\", \"feature_extractor.layer3.11.conv2.weight\", \"feature_extractor.layer3.11.bn2.weight\", \"feature_extractor.layer3.11.bn2.bias\", \"feature_extractor.layer3.11.bn2.running_mean\", \"feature_extractor.layer3.11.bn2.running_var\", \"feature_extractor.layer3.11.conv3.weight\", \"feature_extractor.layer3.11.bn3.weight\", \"feature_extractor.layer3.11.bn3.bias\", \"feature_extractor.layer3.11.bn3.running_mean\", \"feature_extractor.layer3.11.bn3.running_var\", \"feature_extractor.layer3.12.conv1.weight\", \"feature_extractor.layer3.12.bn1.weight\", \"feature_extractor.layer3.12.bn1.bias\", \"feature_extractor.layer3.12.bn1.running_mean\", \"feature_extractor.layer3.12.bn1.running_var\", \"feature_extractor.layer3.12.conv2.weight\", \"feature_extractor.layer3.12.bn2.weight\", \"feature_extractor.layer3.12.bn2.bias\", \"feature_extractor.layer3.12.bn2.running_mean\", \"feature_extractor.layer3.12.bn2.running_var\", \"feature_extractor.layer3.12.conv3.weight\", \"feature_extractor.layer3.12.bn3.weight\", \"feature_extractor.layer3.12.bn3.bias\", \"feature_extractor.layer3.12.bn3.running_mean\", \"feature_extractor.layer3.12.bn3.running_var\", \"feature_extractor.layer3.13.conv1.weight\", \"feature_extractor.layer3.13.bn1.weight\", \"feature_extractor.layer3.13.bn1.bias\", \"feature_extractor.layer3.13.bn1.running_mean\", \"feature_extractor.layer3.13.bn1.running_var\", \"feature_extractor.layer3.13.conv2.weight\", \"feature_extractor.layer3.13.bn2.weight\", \"feature_extractor.layer3.13.bn2.bias\", \"feature_extractor.layer3.13.bn2.running_mean\", \"feature_extractor.layer3.13.bn2.running_var\", \"feature_extractor.layer3.13.conv3.weight\", \"feature_extractor.layer3.13.bn3.weight\", \"feature_extractor.layer3.13.bn3.bias\", \"feature_extractor.layer3.13.bn3.running_mean\", \"feature_extractor.layer3.13.bn3.running_var\", \"feature_extractor.layer3.14.conv1.weight\", \"feature_extractor.layer3.14.bn1.weight\", \"feature_extractor.layer3.14.bn1.bias\", \"feature_extractor.layer3.14.bn1.running_mean\", \"feature_extractor.layer3.14.bn1.running_var\", \"feature_extractor.layer3.14.conv2.weight\", \"feature_extractor.layer3.14.bn2.weight\", \"feature_extractor.layer3.14.bn2.bias\", \"feature_extractor.layer3.14.bn2.running_mean\", \"feature_extractor.layer3.14.bn2.running_var\", \"feature_extractor.layer3.14.conv3.weight\", \"feature_extractor.layer3.14.bn3.weight\", \"feature_extractor.layer3.14.bn3.bias\", \"feature_extractor.layer3.14.bn3.running_mean\", \"feature_extractor.layer3.14.bn3.running_var\", \"feature_extractor.layer3.15.conv1.weight\", \"feature_extractor.layer3.15.bn1.weight\", \"feature_extractor.layer3.15.bn1.bias\", \"feature_extractor.layer3.15.bn1.running_mean\", \"feature_extractor.layer3.15.bn1.running_var\", \"feature_extractor.layer3.15.conv2.weight\", \"feature_extractor.layer3.15.bn2.weight\", \"feature_extractor.layer3.15.bn2.bias\", \"feature_extractor.layer3.15.bn2.running_mean\", \"feature_extractor.layer3.15.bn2.running_var\", \"feature_extractor.layer3.15.conv3.weight\", \"feature_extractor.layer3.15.bn3.weight\", \"feature_extractor.layer3.15.bn3.bias\", \"feature_extractor.layer3.15.bn3.running_mean\", \"feature_extractor.layer3.15.bn3.running_var\", \"feature_extractor.layer3.16.conv1.weight\", \"feature_extractor.layer3.16.bn1.weight\", \"feature_extractor.layer3.16.bn1.bias\", \"feature_extractor.layer3.16.bn1.running_mean\", \"feature_extractor.layer3.16.bn1.running_var\", \"feature_extractor.layer3.16.conv2.weight\", \"feature_extractor.layer3.16.bn2.weight\", \"feature_extractor.layer3.16.bn2.bias\", \"feature_extractor.layer3.16.bn2.running_mean\", \"feature_extractor.layer3.16.bn2.running_var\", \"feature_extractor.layer3.16.conv3.weight\", \"feature_extractor.layer3.16.bn3.weight\", \"feature_extractor.layer3.16.bn3.bias\", \"feature_extractor.layer3.16.bn3.running_mean\", \"feature_extractor.layer3.16.bn3.running_var\", \"feature_extractor.layer3.17.conv1.weight\", \"feature_extractor.layer3.17.bn1.weight\", \"feature_extractor.layer3.17.bn1.bias\", \"feature_extractor.layer3.17.bn1.running_mean\", \"feature_extractor.layer3.17.bn1.running_var\", \"feature_extractor.layer3.17.conv2.weight\", \"feature_extractor.layer3.17.bn2.weight\", \"feature_extractor.layer3.17.bn2.bias\", \"feature_extractor.layer3.17.bn2.running_mean\", \"feature_extractor.layer3.17.bn2.running_var\", \"feature_extractor.layer3.17.conv3.weight\", \"feature_extractor.layer3.17.bn3.weight\", \"feature_extractor.layer3.17.bn3.bias\", \"feature_extractor.layer3.17.bn3.running_mean\", \"feature_extractor.layer3.17.bn3.running_var\", \"feature_extractor.layer3.18.conv1.weight\", \"feature_extractor.layer3.18.bn1.weight\", \"feature_extractor.layer3.18.bn1.bias\", \"feature_extractor.layer3.18.bn1.running_mean\", \"feature_extractor.layer3.18.bn1.running_var\", \"feature_extractor.layer3.18.conv2.weight\", \"feature_extractor.layer3.18.bn2.weight\", \"feature_extractor.layer3.18.bn2.bias\", \"feature_extractor.layer3.18.bn2.running_mean\", \"feature_extractor.layer3.18.bn2.running_var\", \"feature_extractor.layer3.18.conv3.weight\", \"feature_extractor.layer3.18.bn3.weight\", \"feature_extractor.layer3.18.bn3.bias\", \"feature_extractor.layer3.18.bn3.running_mean\", \"feature_extractor.layer3.18.bn3.running_var\", \"feature_extractor.layer3.19.conv1.weight\", \"feature_extractor.layer3.19.bn1.weight\", \"feature_extractor.layer3.19.bn1.bias\", \"feature_extractor.layer3.19.bn1.running_mean\", \"feature_extractor.layer3.19.bn1.running_var\", \"feature_extractor.layer3.19.conv2.weight\", \"feature_extractor.layer3.19.bn2.weight\", \"feature_extractor.layer3.19.bn2.bias\", \"feature_extractor.layer3.19.bn2.running_mean\", \"feature_extractor.layer3.19.bn2.running_var\", \"feature_extractor.layer3.19.conv3.weight\", \"feature_extractor.layer3.19.bn3.weight\", \"feature_extractor.layer3.19.bn3.bias\", \"feature_extractor.layer3.19.bn3.running_mean\", \"feature_extractor.layer3.19.bn3.running_var\", \"feature_extractor.layer3.20.conv1.weight\", \"feature_extractor.layer3.20.bn1.weight\", \"feature_extractor.layer3.20.bn1.bias\", \"feature_extractor.layer3.20.bn1.running_mean\", \"feature_extractor.layer3.20.bn1.running_var\", \"feature_extractor.layer3.20.conv2.weight\", \"feature_extractor.layer3.20.bn2.weight\", \"feature_extractor.layer3.20.bn2.bias\", \"feature_extractor.layer3.20.bn2.running_mean\", \"feature_extractor.layer3.20.bn2.running_var\", \"feature_extractor.layer3.20.conv3.weight\", \"feature_extractor.layer3.20.bn3.weight\", \"feature_extractor.layer3.20.bn3.bias\", \"feature_extractor.layer3.20.bn3.running_mean\", \"feature_extractor.layer3.20.bn3.running_var\", \"feature_extractor.layer3.21.conv1.weight\", \"feature_extractor.layer3.21.bn1.weight\", \"feature_extractor.layer3.21.bn1.bias\", \"feature_extractor.layer3.21.bn1.running_mean\", \"feature_extractor.layer3.21.bn1.running_var\", \"feature_extractor.layer3.21.conv2.weight\", \"feature_extractor.layer3.21.bn2.weight\", \"feature_extractor.layer3.21.bn2.bias\", \"feature_extractor.layer3.21.bn2.running_mean\", \"feature_extractor.layer3.21.bn2.running_var\", \"feature_extractor.layer3.21.conv3.weight\", \"feature_extractor.layer3.21.bn3.weight\", \"feature_extractor.layer3.21.bn3.bias\", \"feature_extractor.layer3.21.bn3.running_mean\", \"feature_extractor.layer3.21.bn3.running_var\", \"feature_extractor.layer3.22.conv1.weight\", \"feature_extractor.layer3.22.bn1.weight\", \"feature_extractor.layer3.22.bn1.bias\", \"feature_extractor.layer3.22.bn1.running_mean\", \"feature_extractor.layer3.22.bn1.running_var\", \"feature_extractor.layer3.22.conv2.weight\", \"feature_extractor.layer3.22.bn2.weight\", \"feature_extractor.layer3.22.bn2.bias\", \"feature_extractor.layer3.22.bn2.running_mean\", \"feature_extractor.layer3.22.bn2.running_var\", \"feature_extractor.layer3.22.conv3.weight\", \"feature_extractor.layer3.22.bn3.weight\", \"feature_extractor.layer3.22.bn3.bias\", \"feature_extractor.layer3.22.bn3.running_mean\", \"feature_extractor.layer3.22.bn3.running_var\", \"feature_extractor.layer3.23.conv1.weight\", \"feature_extractor.layer3.23.bn1.weight\", \"feature_extractor.layer3.23.bn1.bias\", \"feature_extractor.layer3.23.bn1.running_mean\", \"feature_extractor.layer3.23.bn1.running_var\", \"feature_extractor.layer3.23.conv2.weight\", \"feature_extractor.layer3.23.bn2.weight\", \"feature_extractor.layer3.23.bn2.bias\", \"feature_extractor.layer3.23.bn2.running_mean\", \"feature_extractor.layer3.23.bn2.running_var\", \"feature_extractor.layer3.23.conv3.weight\", \"feature_extractor.layer3.23.bn3.weight\", \"feature_extractor.layer3.23.bn3.bias\", \"feature_extractor.layer3.23.bn3.running_mean\", \"feature_extractor.layer3.23.bn3.running_var\", \"feature_extractor.layer3.24.conv1.weight\", \"feature_extractor.layer3.24.bn1.weight\", \"feature_extractor.layer3.24.bn1.bias\", \"feature_extractor.layer3.24.bn1.running_mean\", \"feature_extractor.layer3.24.bn1.running_var\", \"feature_extractor.layer3.24.conv2.weight\", \"feature_extractor.layer3.24.bn2.weight\", \"feature_extractor.layer3.24.bn2.bias\", \"feature_extractor.layer3.24.bn2.running_mean\", \"feature_extractor.layer3.24.bn2.running_var\", \"feature_extractor.layer3.24.conv3.weight\", \"feature_extractor.layer3.24.bn3.weight\", \"feature_extractor.layer3.24.bn3.bias\", \"feature_extractor.layer3.24.bn3.running_mean\", \"feature_extractor.layer3.24.bn3.running_var\", \"feature_extractor.layer3.25.conv1.weight\", \"feature_extractor.layer3.25.bn1.weight\", \"feature_extractor.layer3.25.bn1.bias\", \"feature_extractor.layer3.25.bn1.running_mean\", \"feature_extractor.layer3.25.bn1.running_var\", \"feature_extractor.layer3.25.conv2.weight\", \"feature_extractor.layer3.25.bn2.weight\", \"feature_extractor.layer3.25.bn2.bias\", \"feature_extractor.layer3.25.bn2.running_mean\", \"feature_extractor.layer3.25.bn2.running_var\", \"feature_extractor.layer3.25.conv3.weight\", \"feature_extractor.layer3.25.bn3.weight\", \"feature_extractor.layer3.25.bn3.bias\", \"feature_extractor.layer3.25.bn3.running_mean\", \"feature_extractor.layer3.25.bn3.running_var\", \"feature_extractor.layer3.26.conv1.weight\", \"feature_extractor.layer3.26.bn1.weight\", \"feature_extractor.layer3.26.bn1.bias\", \"feature_extractor.layer3.26.bn1.running_mean\", \"feature_extractor.layer3.26.bn1.running_var\", \"feature_extractor.layer3.26.conv2.weight\", \"feature_extractor.layer3.26.bn2.weight\", \"feature_extractor.layer3.26.bn2.bias\", \"feature_extractor.layer3.26.bn2.running_mean\", \"feature_extractor.layer3.26.bn2.running_var\", \"feature_extractor.layer3.26.conv3.weight\", \"feature_extractor.layer3.26.bn3.weight\", \"feature_extractor.layer3.26.bn3.bias\", \"feature_extractor.layer3.26.bn3.running_mean\", \"feature_extractor.layer3.26.bn3.running_var\", \"feature_extractor.layer3.27.conv1.weight\", \"feature_extractor.layer3.27.bn1.weight\", \"feature_extractor.layer3.27.bn1.bias\", \"feature_extractor.layer3.27.bn1.running_mean\", \"feature_extractor.layer3.27.bn1.running_var\", \"feature_extractor.layer3.27.conv2.weight\", \"feature_extractor.layer3.27.bn2.weight\", \"feature_extractor.layer3.27.bn2.bias\", \"feature_extractor.layer3.27.bn2.running_mean\", \"feature_extractor.layer3.27.bn2.running_var\", \"feature_extractor.layer3.27.conv3.weight\", \"feature_extractor.layer3.27.bn3.weight\", \"feature_extractor.layer3.27.bn3.bias\", \"feature_extractor.layer3.27.bn3.running_mean\", \"feature_extractor.layer3.27.bn3.running_var\", \"feature_extractor.layer3.28.conv1.weight\", \"feature_extractor.layer3.28.bn1.weight\", \"feature_extractor.layer3.28.bn1.bias\", \"feature_extractor.layer3.28.bn1.running_mean\", \"feature_extractor.layer3.28.bn1.running_var\", \"feature_extractor.layer3.28.conv2.weight\", \"feature_extractor.layer3.28.bn2.weight\", \"feature_extractor.layer3.28.bn2.bias\", \"feature_extractor.layer3.28.bn2.running_mean\", \"feature_extractor.layer3.28.bn2.running_var\", \"feature_extractor.layer3.28.conv3.weight\", \"feature_extractor.layer3.28.bn3.weight\", \"feature_extractor.layer3.28.bn3.bias\", \"feature_extractor.layer3.28.bn3.running_mean\", \"feature_extractor.layer3.28.bn3.running_var\", \"feature_extractor.layer3.29.conv1.weight\", \"feature_extractor.layer3.29.bn1.weight\", \"feature_extractor.layer3.29.bn1.bias\", \"feature_extractor.layer3.29.bn1.running_mean\", \"feature_extractor.layer3.29.bn1.running_var\", \"feature_extractor.layer3.29.conv2.weight\", \"feature_extractor.layer3.29.bn2.weight\", \"feature_extractor.layer3.29.bn2.bias\", \"feature_extractor.layer3.29.bn2.running_mean\", \"feature_extractor.layer3.29.bn2.running_var\", \"feature_extractor.layer3.29.conv3.weight\", \"feature_extractor.layer3.29.bn3.weight\", \"feature_extractor.layer3.29.bn3.bias\", \"feature_extractor.layer3.29.bn3.running_mean\", \"feature_extractor.layer3.29.bn3.running_var\", \"feature_extractor.layer3.30.conv1.weight\", \"feature_extractor.layer3.30.bn1.weight\", \"feature_extractor.layer3.30.bn1.bias\", \"feature_extractor.layer3.30.bn1.running_mean\", \"feature_extractor.layer3.30.bn1.running_var\", \"feature_extractor.layer3.30.conv2.weight\", \"feature_extractor.layer3.30.bn2.weight\", \"feature_extractor.layer3.30.bn2.bias\", \"feature_extractor.layer3.30.bn2.running_mean\", \"feature_extractor.layer3.30.bn2.running_var\", \"feature_extractor.layer3.30.conv3.weight\", \"feature_extractor.layer3.30.bn3.weight\", \"feature_extractor.layer3.30.bn3.bias\", \"feature_extractor.layer3.30.bn3.running_mean\", \"feature_extractor.layer3.30.bn3.running_var\", \"feature_extractor.layer3.31.conv1.weight\", \"feature_extractor.layer3.31.bn1.weight\", \"feature_extractor.layer3.31.bn1.bias\", \"feature_extractor.layer3.31.bn1.running_mean\", \"feature_extractor.layer3.31.bn1.running_var\", \"feature_extractor.layer3.31.conv2.weight\", \"feature_extractor.layer3.31.bn2.weight\", \"feature_extractor.layer3.31.bn2.bias\", \"feature_extractor.layer3.31.bn2.running_mean\", \"feature_extractor.layer3.31.bn2.running_var\", \"feature_extractor.layer3.31.conv3.weight\", \"feature_extractor.layer3.31.bn3.weight\", \"feature_extractor.layer3.31.bn3.bias\", \"feature_extractor.layer3.31.bn3.running_mean\", \"feature_extractor.layer3.31.bn3.running_var\", \"feature_extractor.layer3.32.conv1.weight\", \"feature_extractor.layer3.32.bn1.weight\", \"feature_extractor.layer3.32.bn1.bias\", \"feature_extractor.layer3.32.bn1.running_mean\", \"feature_extractor.layer3.32.bn1.running_var\", \"feature_extractor.layer3.32.conv2.weight\", \"feature_extractor.layer3.32.bn2.weight\", \"feature_extractor.layer3.32.bn2.bias\", \"feature_extractor.layer3.32.bn2.running_mean\", \"feature_extractor.layer3.32.bn2.running_var\", \"feature_extractor.layer3.32.conv3.weight\", \"feature_extractor.layer3.32.bn3.weight\", \"feature_extractor.layer3.32.bn3.bias\", \"feature_extractor.layer3.32.bn3.running_mean\", \"feature_extractor.layer3.32.bn3.running_var\", \"feature_extractor.layer3.33.conv1.weight\", \"feature_extractor.layer3.33.bn1.weight\", \"feature_extractor.layer3.33.bn1.bias\", \"feature_extractor.layer3.33.bn1.running_mean\", \"feature_extractor.layer3.33.bn1.running_var\", \"feature_extractor.layer3.33.conv2.weight\", \"feature_extractor.layer3.33.bn2.weight\", \"feature_extractor.layer3.33.bn2.bias\", \"feature_extractor.layer3.33.bn2.running_mean\", \"feature_extractor.layer3.33.bn2.running_var\", \"feature_extractor.layer3.33.conv3.weight\", \"feature_extractor.layer3.33.bn3.weight\", \"feature_extractor.layer3.33.bn3.bias\", \"feature_extractor.layer3.33.bn3.running_mean\", \"feature_extractor.layer3.33.bn3.running_var\", \"feature_extractor.layer3.34.conv1.weight\", \"feature_extractor.layer3.34.bn1.weight\", \"feature_extractor.layer3.34.bn1.bias\", \"feature_extractor.layer3.34.bn1.running_mean\", \"feature_extractor.layer3.34.bn1.running_var\", \"feature_extractor.layer3.34.conv2.weight\", \"feature_extractor.layer3.34.bn2.weight\", \"feature_extractor.layer3.34.bn2.bias\", \"feature_extractor.layer3.34.bn2.running_mean\", \"feature_extractor.layer3.34.bn2.running_var\", \"feature_extractor.layer3.34.conv3.weight\", \"feature_extractor.layer3.34.bn3.weight\", \"feature_extractor.layer3.34.bn3.bias\", \"feature_extractor.layer3.34.bn3.running_mean\", \"feature_extractor.layer3.34.bn3.running_var\", \"feature_extractor.layer3.35.conv1.weight\", \"feature_extractor.layer3.35.bn1.weight\", \"feature_extractor.layer3.35.bn1.bias\", \"feature_extractor.layer3.35.bn1.running_mean\", \"feature_extractor.layer3.35.bn1.running_var\", \"feature_extractor.layer3.35.conv2.weight\", \"feature_extractor.layer3.35.bn2.weight\", \"feature_extractor.layer3.35.bn2.bias\", \"feature_extractor.layer3.35.bn2.running_mean\", \"feature_extractor.layer3.35.bn2.running_var\", \"feature_extractor.layer3.35.conv3.weight\", \"feature_extractor.layer3.35.bn3.weight\", \"feature_extractor.layer3.35.bn3.bias\", \"feature_extractor.layer3.35.bn3.running_mean\", \"feature_extractor.layer3.35.bn3.running_var\", \"feature_extractor.layer4.0.conv3.weight\", \"feature_extractor.layer4.0.bn3.weight\", \"feature_extractor.layer4.0.bn3.bias\", \"feature_extractor.layer4.0.bn3.running_mean\", \"feature_extractor.layer4.0.bn3.running_var\", \"feature_extractor.layer4.1.conv3.weight\", \"feature_extractor.layer4.1.bn3.weight\", \"feature_extractor.layer4.1.bn3.bias\", \"feature_extractor.layer4.1.bn3.running_mean\", \"feature_extractor.layer4.1.bn3.running_var\", \"feature_extractor.layer4.2.conv1.weight\", \"feature_extractor.layer4.2.bn1.weight\", \"feature_extractor.layer4.2.bn1.bias\", \"feature_extractor.layer4.2.bn1.running_mean\", \"feature_extractor.layer4.2.bn1.running_var\", \"feature_extractor.layer4.2.conv2.weight\", \"feature_extractor.layer4.2.bn2.weight\", \"feature_extractor.layer4.2.bn2.bias\", \"feature_extractor.layer4.2.bn2.running_mean\", \"feature_extractor.layer4.2.bn2.running_var\", \"feature_extractor.layer4.2.conv3.weight\", \"feature_extractor.layer4.2.bn3.weight\", \"feature_extractor.layer4.2.bn3.bias\", \"feature_extractor.layer4.2.bn3.running_mean\", \"feature_extractor.layer4.2.bn3.running_var\". \n\tsize mismatch for feature_extractor.layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for feature_extractor.layer1.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for feature_extractor.layer2.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for feature_extractor.layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for feature_extractor.layer2.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for feature_extractor.layer3.0.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 512, 1, 1]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for feature_extractor.layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for feature_extractor.layer3.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for feature_extractor.layer4.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for feature_extractor.layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for feature_extractor.layer4.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for feature_extractor.fc.weight: copying a param with shape torch.Size([1000, 512]) from checkpoint, the shape in current model is torch.Size([1000, 2048])."
     ]
    }
   ],
   "source": [
    "gender_model=resnet18()\n",
    "gender_model.fc=nn.Linear(512,2)\n",
    "gender_wt=torch.load('weight/classification/UTK_gender_best_model.pt')\n",
    "\n",
    "emotion_model=resnet.EmotionModel(phase='test')\n",
    "emotion_wt=torch.load('weight/classification/emotion128_resnet18.pt')\n",
    "\n",
    "age_model=resnet.AgeModel(phase='test')\n",
    "age_wt=torch.load('weight/classification/age128_4class_merge_resnet18.pt')\n",
    "\n",
    "\n",
    "gender_model.to(device)\n",
    "emotion_model.to(device)\n",
    "age_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "emotion_model.load_state_dict(emotion_wt)\n",
    "gender_model.load_state_dict(gender_wt)\n",
    "age_model.load_state_dict(age_wt)\n",
    "\n",
    "\n",
    "emotion_model.eval()\n",
    "gender_model.eval()\n",
    "age_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not pretrained model loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiTaskModel(\n",
       "  (feature_extractor): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (gender_fc): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (emo_fc): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=7, bias=True)\n",
       "  )\n",
       "  (age_fc): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MTL_model=MTL_model.MultiTaskModel(phase='test')\n",
    "MTL_wt=torch.load('weight/MTL/resnet18_MTL_212.pt')\n",
    "MTL_model.to(device)\n",
    "MTL_model.load_state_dict(MTL_wt)\n",
    "MTL_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length =30\n",
    "unique_list = []\n",
    "while len(unique_list) < length:\n",
    "    num = random.randint(1, 9000)\n",
    "    if num not in unique_list:\n",
    "        unique_list.append(num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maodwndhks\u001b[0m (\u001b[33mkookmin_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joowan/Desktop/face_pr/wandb/run-20230508_162900-qighjr2x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/qighjr2x' target=\"_blank\">comic-butterfly-27</a></strong> to <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result' target=\"_blank\">https://wandb.ai/kookmin_ai/single%20tasks%20result</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/qighjr2x' target=\"_blank\">https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/qighjr2x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='single tasks result',entity='kookmin_ai')\n",
    "wandb.run.name=(f'emotion_test')\n",
    "\n",
    "example_images=[]\n",
    "for i in unique_list:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        emo_img=test_emo_dataset[i][0]\n",
    "        emo_answer=emo_label[test_emo_dataset[i][1]]\n",
    "        emo_img=emo_img.to(device)\n",
    "        emo_img=emo_img.unsqueeze(0)\n",
    "        \n",
    "        start=time.time()\n",
    "        emo_output=emotion_model(emo_img)\n",
    "        infer_time=time.time()-start\n",
    "        \n",
    "        emo_pred=emo_output.argmax(1,keepdim=True)\n",
    "        emotion=emo_label[emo_pred.item()]\n",
    "    example_images.append(wandb.Image(\n",
    "                    emo_img, caption=f'Pred:{emotion},  Answer:{emo_answer}'))\n",
    "wandb.log({\"Image\": example_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qighjr2x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-butterfly-27</strong> at: <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/qighjr2x' target=\"_blank\">https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/qighjr2x</a><br/>Synced 5 W&B file(s), 30 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_162900-qighjr2x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qighjr2x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joowan/Desktop/face_pr/wandb/run-20230508_162945-87gh6llc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/87gh6llc' target=\"_blank\">revived-puddle-28</a></strong> to <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result' target=\"_blank\">https://wandb.ai/kookmin_ai/single%20tasks%20result</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/87gh6llc' target=\"_blank\">https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/87gh6llc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='single tasks result',entity='kookmin_ai')\n",
    "wandb.run.name=(f'age_test')\n",
    "\n",
    "example_images=[]\n",
    "for i in unique_list:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        age_img=test_age_dataset[i][0]\n",
    "        age_answer=age_label[test_age_dataset[i][1]]\n",
    "        age_img=age_img.to(device)\n",
    "        age_img=age_img.unsqueeze(0)\n",
    "        \n",
    "        start=time.time()\n",
    "        age_output=age_model(age_img)\n",
    "        infer_time=time.time()-start\n",
    "        \n",
    "        age_pred=age_output.argmax(1,keepdim=True)\n",
    "        age=age_label[age_pred.item()]\n",
    "    example_images.append(wandb.Image(\n",
    "                    age_img, caption=f'Pred:{age},  Answer:{age_answer}'))\n",
    "wandb.log({\"Image\": example_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:87gh6llc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-puddle-28</strong> at: <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/87gh6llc' target=\"_blank\">https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/87gh6llc</a><br/>Synced 5 W&B file(s), 30 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_162945-87gh6llc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:87gh6llc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joowan/Desktop/face_pr/wandb/run-20230508_162953-bh2jndn6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/bh2jndn6' target=\"_blank\">wise-blaze-29</a></strong> to <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result' target=\"_blank\">https://wandb.ai/kookmin_ai/single%20tasks%20result</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/bh2jndn6' target=\"_blank\">https://wandb.ai/kookmin_ai/single%20tasks%20result/runs/bh2jndn6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='single tasks result',entity='kookmin_ai')\n",
    "wandb.run.name=(f'gender_test')\n",
    "\n",
    "example_images=[]\n",
    "for i in unique_list:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        gender_img=test_gender_dataset[i][0]\n",
    "        gender_answer=gender_label[test_gender_dataset[i][1]]\n",
    "        gender_img=gender_img.to(device)\n",
    "        gender_img=gender_img.unsqueeze(0)\n",
    "        \n",
    "        start=time.time()\n",
    "        gender_output=gender_model(gender_img)\n",
    "        infer_time=time.time()-start\n",
    "        \n",
    "        gender_pred=gender_output.argmax(1,keepdim=True)\n",
    "        gender=gender_label[gender_pred.item()]\n",
    "    example_images.append(wandb.Image(\n",
    "                    gender_img, caption=f'Pred:{gender},  Answer:{gender_answer}'))\n",
    "wandb.log({\"Image\": example_images})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:66ihvkpb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-pond-10</strong> at: <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/66ihvkpb' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/66ihvkpb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_161423-66ihvkpb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:66ihvkpb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joowan/Desktop/face_pr/wandb/run-20230508_161444-ua614orq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/ua614orq' target=\"_blank\">firm-snowflake-11</a></strong> to <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/ua614orq' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/ua614orq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='multi tasks result',entity='kookmin_ai')\n",
    "wandb.run.name=(f'emotion_test')\n",
    "\n",
    "example_images=[]\n",
    "for i in unique_list:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        emo_img=test_emo_dataset[i][0]\n",
    "        emo_answer=emo_label[test_emo_dataset[i][1]]\n",
    "        emo_img=emo_img.to(device)\n",
    "        emo_img=emo_img.unsqueeze(0)\n",
    "        \n",
    "        start=time.time()\n",
    "        outputs=MTL_model(emo_img)\n",
    "        emo_output=outputs[1]\n",
    "        infer_time=time.time()-start\n",
    "        \n",
    "        emo_pred=emo_output.argmax(1,keepdim=True)\n",
    "        emotion=emo_label[emo_pred.item()]\n",
    "    example_images.append(wandb.Image(\n",
    "                    emo_img, caption=f'Pred:{emotion},  Answer:{emo_answer}'))\n",
    "wandb.log({\"Image\": example_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:04pm9yq1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-violet-6</strong> at: <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/04pm9yq1' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/04pm9yq1</a><br/>Synced 5 W&B file(s), 30 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_160439-04pm9yq1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:04pm9yq1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joowan/Desktop/face_pr/wandb/run-20230508_160448-yb17vw4v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/yb17vw4v' target=\"_blank\">fine-water-7</a></strong> to <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/yb17vw4v' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/yb17vw4v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='multi tasks result',entity='kookmin_ai')\n",
    "wandb.run.name=(f'age_test')\n",
    "\n",
    "example_images=[]\n",
    "for i in unique_list:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        age_img=test_age_dataset[i][0]\n",
    "        age_answer=age_label[test_age_dataset[i][1]]\n",
    "        age_img=age_img.to(device)\n",
    "        age_img=age_img.unsqueeze(0)\n",
    "        \n",
    "        start=time.time()\n",
    "        outputs=MTL_model(age_img)\n",
    "        age_output=outputs[2]\n",
    "        infer_time=time.time()-start\n",
    "        \n",
    "        age_pred=age_output.argmax(1,keepdim=True)\n",
    "        age=age_label[age_pred.item()]\n",
    "    example_images.append(wandb.Image(\n",
    "                    age_img, caption=f'Pred:{age},  Answer:{age_answer}'))\n",
    "wandb.log({\"Image\": example_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ua614orq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-snowflake-11</strong> at: <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/ua614orq' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/ua614orq</a><br/>Synced 5 W&B file(s), 30 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_161444-ua614orq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ua614orq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joowan/Desktop/face_pr/wandb/run-20230508_161504-ztgzn90t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/ztgzn90t' target=\"_blank\">clean-pyramid-12</a></strong> to <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/ztgzn90t' target=\"_blank\">https://wandb.ai/kookmin_ai/multi%20tasks%20result/runs/ztgzn90t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='multi tasks result',entity='kookmin_ai')\n",
    "wandb.run.name=(f'gender_test')\n",
    "\n",
    "example_images=[]\n",
    "for i in unique_list:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        gender_img=test_gender_dataset[i][0]\n",
    "        gender_answer=gender_label[test_gender_dataset[i][1]]\n",
    "        gender_img=gender_img.to(device)\n",
    "        gender_img=gender_img.unsqueeze(0)\n",
    "        \n",
    "        start=time.time()\n",
    "        outputs=MTL_model(gender_img)\n",
    "        gender_output=outputs[0]\n",
    "        infer_time=time.time()-start\n",
    "        \n",
    "        gender_pred=gender_output.argmax(1,keepdim=True)\n",
    "        gender=gender_label[gender_pred.item()]\n",
    "    example_images.append(wandb.Image(\n",
    "                    gender_img, caption=f'Pred:{gender},  Answer:{gender_answer}'))\n",
    "wandb.log({\"Image\": example_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
